{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDkn--vjlx9I"
      },
      "source": [
        "# HW-5: Malware Classification (Due 5th January, 2023)\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "Suppose your company is struggling with a series of computer virus attacks for the past several months. The viruses were grouped into a few types with some effort. However, it takes a long time to sort out what kind of virus it is when been hit with. Thus, as a senior IT department member, you undertook a project to classify the virus as quickly as possible. You've been given a dataset of the features that may be handy (or not), and  also the associated virus type (target variable). \n",
        "\n",
        "You are supposed to try different classification methods and apply best practices we have seen in the lectures such as grid search, cross validation, regularization etc. To increase your grade you can add more elaboration such as using ensembling or exploiting feature selection/extraction techniques. **An evaluation rubric is provided.**\n",
        "\n",
        "Please prepare a python notebook that describes the steps, present the results as well as your comments. \n",
        "\n",
        "You can download the data (csv file) [here](https://drive.google.com/file/d/1yxbibzUU8bjOyChDVFPfQ4viLduYdk29/view?usp=sharing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GkII1TlA-QA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Kutay Ã–zbay 270201017 HW5\n",
        "\n",
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import GenericUnivariateSelect\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In this work listed classification methods are used;\n",
        "#Logistic Regression\n",
        "#Decision Tree\n",
        "#K-Nearest Neighbours\n",
        "#Random Forest\n",
        "#Support Vector Machine\n",
        "#KMeans\n",
        "#And Ensembling between LR K-NN and Decision Tree"
      ],
      "metadata": {
        "id": "cHPgVsl5KN43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this work listed classification methods are used;\n",
        "1- Logistic Regression\n",
        "2- Decision Tree\n",
        "3-"
      ],
      "metadata": {
        "id": "9uuGyu01J7H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "-eC_Y8Z_BFKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading Data\n",
        "virus_df = pd.read_csv(io.StringIO(uploaded['hw5_data.csv'].decode('utf-8')))\n"
      ],
      "metadata": {
        "id": "u21eD9bpBApU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting data into target values and features\n",
        "y = virus_df['target']\n",
        "X = virus_df.drop('target', axis=1)\n"
      ],
      "metadata": {
        "id": "t8_fJ5-lBCka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Selection using mutual information with Filter Methods\n",
        "mf_select = GenericUnivariateSelect(score_func=mutual_info_classif, mode=\"k_best\", param=100)\n",
        "mf_select.fit(X, y)\n",
        "X_mf = mf_select.transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mf,y,test_size = 0.2)"
      ],
      "metadata": {
        "id": "5oDZpnm3BDc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression with hyper parameter search \n",
        "lg_reg = modeling_pipeline = Pipeline([('scaling', StandardScaler()),\n",
        "                                       ('model', LogisticRegression(solver='liblinear'))])\n",
        "\n",
        "params = {'model__C': [0.1, 1, 10, 20, 50, 100, 1000]}\n",
        "\n",
        "virus_search = GridSearchCV(estimator=lg_reg, param_grid=params, scoring='accuracy', cv=5, refit=True)\n",
        "\n",
        "virus_search.fit(X_train,y_train)\n",
        "\n",
        "print(virus_search.best_estimator_)\n",
        "\n",
        "print(f'Validation score: {virus_search.best_score_:.2%}')\n",
        "print('Test score:' )\n",
        "print(virus_search.score(X_test, y_test))\n",
        "\n",
        "#Below there is a version with LDA although LDA lowers performence it makes algorithm faster.\n",
        "\"\"\"\n",
        "p = Pipeline([('lda', LDA()),\n",
        "              ('model', LogisticRegression(solver='liblinear'))\n",
        "             ])\n",
        "\n",
        "params = {'model__C': [0.1, 1, 10, 20, 50, 100, 1000], 'lda__n_components': [1, 2, 3, 4, 5, 6, 7, 8]}\n",
        "\n",
        "virus_search = GridSearchCV(p, param_grid=params, scoring='accuracy', cv=5, refit=True)\n",
        "\n",
        "virus_search.fit(X_train,y_train)\n",
        "\n",
        "print(virus_search.best_estimator_)\n",
        "\n",
        "print(f'Validation score: {virus_search.best_score_:.2%}')\n",
        "print('Test score:' )\n",
        "print(virus_search.score(X_test, y_test))\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6uLh2LyrBPq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree with using GridSearch for trying hyperparameters systematically\n",
        "std_slc = StandardScaler()\n",
        "dec_tree = DecisionTreeClassifier()\n",
        "\n",
        "pipe = Pipeline(steps=[('std_slc', std_slc),\n",
        "                           ('dec_tree', dec_tree)])\n",
        "\n",
        "criterion = ['gini', 'entropy']\n",
        "max_depth = [2,4,6,8,10,12]\n",
        "\n",
        "parameters = dict(dec_tree__criterion=criterion,\n",
        "                  dec_tree__max_depth=max_depth)\n",
        "\n",
        "tree_GS = GridSearchCV(pipe, parameters)\n",
        "tree_GS.fit(X_train, y_train)\n",
        "\n",
        "print('Best Criterion:',tree_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
        "print('Best max_depth:', tree_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
        "print(); print(tree_GS.best_estimator_.get_params()['dec_tree'])\n",
        "\n",
        "print(tree_GS.predict(X_test)==y_test)"
      ],
      "metadata": {
        "id": "-IPcXxmlBVN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#K-Nearest Neighbours\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "k_range = list(range(1, 31))\n",
        "\n",
        "param_grid = dict(n_neighbors=k_range)\n",
        "  \n",
        "# defining parameter range\n",
        "knn_grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', return_train_score=False,verbose=1)\n",
        "  \n",
        "# fitting the model for grid search\n",
        "grid_search= knn_grid.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))\n",
        "\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=grid_search.best_params_.get('n_neighbors'))\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_test_hat=knn.predict(X_test) \n",
        "\n",
        "test_accuracy=accuracy_score(y_test,y_test_hat)*100\n",
        "\n",
        "print(\"Accuracy for our testing dataset with tuning is : {:.2f}%\".format(test_accuracy))"
      ],
      "metadata": {
        "id": "F9Y6RbooBXhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "\n",
        "# First, lets vote among all 3 model types\n",
        "classifiers = [DecisionTreeClassifier(),\n",
        "               BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=0.5, oob_score=True),\n",
        "               RandomForestClassifier(n_estimators=100, oob_score=True)]\n",
        "names = [\"DecisionTree\", \"Bagged Trees\", \"RandomForest\"]\n",
        "\n",
        "\n",
        "for model, m_name in zip(classifiers, names):\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"\\t\", m_name, accuracy_score(y_test, model.predict(X_test)))  "
      ],
      "metadata": {
        "id": "ELQMCbmbBfPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Support Vector Machine\n",
        "\n",
        "svm_m = modeling_pipeline = Pipeline([('scaling', StandardScaler()),\n",
        "                                       ('model', SVC())])\n",
        "\n",
        "param_grid = [\n",
        "  {'model__C': [0.01, 0.1, 1, 10, 100, 1000], 'model__kernel': ['linear','rbf']}\n",
        " ]\n",
        "\n",
        "svm_results = GridSearchCV(estimator=svm_m, param_grid=param_grid, scoring='accuracy', refit=True, cv=5)\n",
        "svm_results = svm_results.fit(X_train, y_train)\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(svm_results, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "svm_score = svm_results.score(X_test, y_test)\n",
        "\n",
        "print(f'Support Vector Machine Score: {svm_score:.2%}')"
      ],
      "metadata": {
        "id": "HvzlM2FnBiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KMeans\n",
        "\n",
        "#Elbow method\n",
        "scores = []\n",
        "for i in range(1,11):\n",
        "    k_means = KMeans(n_clusters=i)\n",
        "    k_means.fit(X_train)\n",
        "    scores.append( -k_means.score(X_train) )\n",
        "\n",
        "#Plot elbow curve as line\n",
        "plt.plot(np.arange(1,11),scores)\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('Clusters (k)')\n",
        "plt.show()\n",
        "\n",
        "k_means_optimum = KMeans(n_clusters = 8)\n",
        "y = k_means_optimum.fit_predict(X_test)\n",
        "\n",
        "sum = 0\n",
        "for i in range(len(y)):\n",
        "    if y[i] == y_test[i]:\n",
        "        sum += 1\n",
        "print(sum / len(y))"
      ],
      "metadata": {
        "id": "WpNQqYCBBknH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensembling\n",
        "\n",
        "#Using LR K-NN and Decision Tree\n",
        "classifiers = [LogisticRegression(solver='liblinear'), KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier()]\n",
        "names = [\"LR\", \"5-NN\", \"DecisionTree\"]\n",
        "\n",
        "\n",
        "classifiers.append( VotingClassifier([ (type(x).__name__, x) for x in classifiers ]) )\n",
        "names.append(\"Vote(LR,5-NN,DT)\")\n",
        "\n",
        "classifiers.append( VotingClassifier([ (x, LogisticRegression(solver='liblinear', multi_class='auto') ) for x in \"abc\"  ]) )\n",
        "names.append(\"Vote(LRx3)\")\n",
        "\n",
        "classifiers.append( VotingClassifier([ (str(x), KNeighborsClassifier() ) for x in range(3) ]) )\n",
        "names.append(\"Vote(5-NNx3)\")\n",
        "\n",
        "  \n",
        "\n",
        "for model, m_name in zip(classifiers, names):\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"\\t\", m_name, accuracy_score(y_test, model.predict(X_test)))"
      ],
      "metadata": {
        "id": "HbuGKpKACPWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}